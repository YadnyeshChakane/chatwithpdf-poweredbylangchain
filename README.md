# chatwithpdf-poweredbylangchainü¶úüîó
Chat with PDF - Powered by LangChain
This project is an LLM-powered chatbot that allows you to interact with PDF files using natural language queries. It is built using the following technologies:

**Streamlit:** 
A Python library for building interactive web applications.

**LangChain:**
LangChain is a framework designed to simplify the creation of applications using large language models (LLMs).

**OpenAI LLM model:**
A language model developed by OpenAI.

**Aboutüìñ:**

The "Chat with PDF" app provides an interface for uploading a PDF file and asking questions or querying the content of the document. It utilizes LangChain and the OpenAI LLM model to process the queries and provide relevant answers based on the content of the PDF.

**How to Useüîç:**

Install the required dependencies by running pip install -r requirements.txt.
Run the script app.py using python app.py (streamlit run app.py)
The web application will start, and you can access it in your browser.
Remember to use your own OpenAI API key and paste it in the .env file.
Upload a PDF file using the file upload button.
Enter your questions or queries about the PDF content in the text input box.
The chatbot will process your query and provide relevant answers based on the content of the PDF.

**Dependenciesüî®:**

os.path,
os,
streamlit,
pickle,
dotenv,
PyPDF2,
streamlit_extras,
langchain,
langchain.embeddings.openai,
langchain.vectorstores,
langchain.llms,
langchain.chains.question_answering

**How it Works?ü§î**

When you run the app, it creates a web interface using Streamlit where you can interact with it.
You can upload a PDF file using the file upload button in the app. The uploaded file is then processed using the PyPDF2 library to extract the text content from the PDF.

The extracted text is split into smaller chunks using the RecursiveCharacterTextSplitter from LangChain. This helps in processing and searching the text efficiently.

The next step involves generating embeddings for the text chunks. Embeddings are numerical representations of text that capture its semantic meaning. The app uses the OpenAIEmbeddings class from LangChain to generate embeddings for the text chunks.

The embeddings for the text chunks are stored in a VectorStore, which is created using the FAISS library. The VectorStore enables efficient similarity search based on the embeddings.

After the PDF file is processed and the VectorStore is created, you can enter your questions or queries about the PDF content in the text input box provided by the app.

When you enter a query, the app uses the VectorStore to perform a similarity search and retrieve the most relevant text chunks based on the query. The number of retrieved text chunks can be configured (in this case, it retrieves the top 3 chunks).

The app then uses the LangChain library to create an OpenAI language model (LLM) instance. The LLM is a powerful model that can understand and generate human-like text.

The app loads a pre-trained question-answering chain from LangChain using the load_qa_chain function. This chain is responsible for processing the input documents (text chunks) and the user's question/query.

The question-answering chain is executed with the input documents and the user's query. The chain processes the documents and generates a response that best answers the user's query.

The response generated by the question-answering chain is then displayed in the app interface for you to see.

